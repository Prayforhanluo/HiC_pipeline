#!/usr/bin/env python

# Created on Tue Dec 16 10:22:41 2014

# Author: XiaoTao Wang
# Organization: HuaZhong Agricultural University

## Required Modules
import os, sys, argparse, logging, logging.handlers, glob

try:
    import numpy as np
except ImportError:
    pass

def getargs():
    ## Construct an ArgumentParser object for command-line arguments
    parser = argparse.ArgumentParser(description = '''This software is based on hiclib
                                    (https://bitbucket.org/mirnylab/hiclib), a comprehensive
                                    Python package for Hi-C data analysis. Before running this
                                    program, you should: 1.Install all required software or
                                    libraries; 2.Re-organize your directory arrangements; (A
                                    data folder with all genome and sequencing data placed
                                    here, and a separate working directory); 3.Place genome
                                    data under the data folder, each named after the corresponding
                                    genome name. Genome sequences should be stored chromosome
                                    by chromosome in FASTA format. The gap file is also needed,
                                    but if it is not provided, we will generate a dummy one;
                                    4.Construct a metadata file describing your sequencing data
                                    under the working directory. Four columns are required: prefix
                                    of SRA file name, cell line name, biological replicate label,
                                    and restriction enzyme name. An example file is distributed
                                    along with this software, please check it.''',
                                    formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    
    # Version
    parser.add_argument('-v', '--version', action = 'version', version = '%(prog)s 0.3.9',
                        help = 'Print version number and exit')
    
    ## One for all
    common = argparse.ArgumentParser(add_help = False)
    common.add_argument('-p', '--dataFolder', default = '.',
                        help = '''Root directory of original data. We recommend placing sequencing
                        and genome data here.''')
    common.add_argument('-g', '--genomeName',
                        help = '''Genome folder name. This folder must be placed under dataFolder.
                        Genome sequences should be stored chromosome by chromosome in FASTA format.
                        If gap file is not contained, we will generate a dummy one.''')
    common.add_argument('-C', '--chroms', nargs = '*', default = ['#', 'X'],
                       help = '''Which chromosomes will be involved. Specially, "#" stands for
                       chromosomes with numerical labels. "--chroms" with zero argument will
                       generate an empty list, in which case all chromosome data will be loaded.''')
    common.add_argument('-T', '--template', default = 'chr%s.fa',
                        help = '''Template of FASTA file names''')
    common.add_argument('-G', '--gapFile', default = 'gap.txt', help = '''Gap file name.''')
    common.add_argument('--logFile', default = 'runHiC.log',
                        help = '''Logging file name.''')
    
    ## Sub-commands
    subparser = parser.add_subparsers(title = 'sub-commands',
                                      description = '''Read pair mapping, filtering, binning
                                      and iterative correction are contained. You can perform
                                      each stage of the analysis separately, or streamline the
                                      pipeline using "pileup" subcommand.''',
                                      dest = 'subcommand')
    ## Iterative Mapping
    iterM = subparser.add_parser('mapping',
                                 parents = [common],
                                 help = '''Map raw pair-end sequencing data to a supplied
                                 genome. Both SRA and FASTQ format are admissible.''',
                                 description = '''An iterative mapping schema is used. The
                                 minimum length is always 25, then the step will be calculated
                                 automatically based on the sequence length. The bowtie2 mapping
                                 software and a fastq-dump tool from SRA toolkit are required.
                                 At least, you should specify --fastqDir, --genomeName,
                                 --bowtiePath, --dataFolder and --metadata yourself.''',
                                 epilog = '''After this command, a BAM folder containing BAM
                                 files for each side of Hi-C molecules and a HDF5 folder containing
                                 hdf5 (dict-like structure format) files for library of matched
                                 Hi-C reads are created under current working directory.''',
                                 formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    iterM.add_argument('-f', '--fastqDir', help = 'Sequencing data folder. Relative path to dataFolder')
    iterM.add_argument('-F', '--Format', default = 'SRA', choices = ['SRA', 'FASTQ'],
                       help = 'Format of the sequencing data.')
    iterM.add_argument('-b', '--bowtiePath', help = 'Path to bowtie2 executable program file.')
    iterM.add_argument('-t', '--threads', type = int, default = 4, help = 'Number bowtie2 threads.')
    iterM.add_argument('-i', '--bowtieIndex',
                       help = '''Path to the bowtie2 genome index. Since the index consists of
                       several files with the different suffices (e.g., hg19.1.bt2, hg19.2.bt.2),
                       provide only the common part. For example, if your genome data hg19.fa
                       and corresponding index files are stored in ~/data/hg19, you need to
                       specify --bowtieIndex as this "--bowtieIndex ~/data/hg19/hg19". When not
                       specified, we will generate one under the genome folder.''')
    iterM.add_argument('-m', '--metadata', default = 'datasets.tsv',
                       help = '''Metadata file describing each SRA file. You should place
                       it under current working directory. Four columns are required: prefix
                       of SRA file name, cell line name, biological replicate label, and
                       restriction enzyme name. An example file is distributed along with
                       this software, please check it.''')
    iterM.add_argument('--cache', default = '/tmp',
                       help = ''''Set the cache folder. Absolute path is needed.''')
    iterM.set_defaults(func = mapping)
    
    ## Merging and Filtering
    removeNoise = subparser.add_parser('filtering',
                                       parents = [common],
                                       help = '''Filtering at the level of aligned read pairs
                                       and restriction fragments. Files from the same experiment
                                       will be merged together at this stage.''',
                                       description = '''PCR duplications, self-ligation products,
                                       unligated "dangling end" products, random breaks, too
                                       large and too small fragments, and fragments with high
                                       cis-to-trans ratio are all taken into account.''',
                                       epilog = '''A folder with one or more hdf5 files containing
                                       fragment-level information are generated under current working
                                       directory after this command is called.''',
                                       formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    removeNoise.add_argument('--HDF5',
                             help = '''Path to the folder with hdf5 files which are generated by
                             mapping command.''')
    removeNoise.add_argument('-m', '--metadata', default = 'datasets.tsv',
                             help = '''Metadata file describing each SRA file. You should place
                             it under current working directory. Four columns are required: prefix
                             of SRA file name, cell line name, biological replicate label, and
                             restriction enzyme name. An example file is distributed along with
                             this software, please check it.''')
    removeNoise.add_argument('--duplicates', action = 'store_true',
                             help = '''Remove read pairs resulting from PCR amplification.''')
    removeNoise.add_argument('--sameFragments', action = 'store_true',
                             help = '''Remove read pairs which located in the same restriction
                             fragments. Two cases are included: self-ligation products and unligated
                             "dangling end" products.''')
    removeNoise.add_argument('--startNearRsite', action = 'store_true',
                             help = '''Remove reads that start within 5 bp near a restriction site.''')
    removeNoise.add_argument('--RandomBreaks', action = 'store_true',
                             help = '''Remove "random breaks" in which corresponding fragments
                             did not arise from normal restriction digestion.''')
    removeNoise.add_argument('--extremeFragments', action = 'store_true',
                             help = '''Remove too large and too small fragments.''')
    removeNoise.add_argument('--cistotrans', action = 'store_true',
                             help = '''Remove certain fraction of fragments with the greatest
                             number of reads.''')
    removeNoise.add_argument('-l', '--level', type = int, default = 2, choices = [1, 2],
                             help = '''Set merging level. 1: hdf5 files from the same biological
                             replicate will be merged, 2: hdf5 files from the same cell line will also be
                             merged.''')
    removeNoise.set_defaults(func = filtering)
    
    ## Binning
    binReads = subparser.add_parser('binning',
                                    parents = [common],
                                    help = '''Bin filtered reads at certain resolution.''',
                                    description = '''For varying resolutions, three modes are
                                    provided, just choose a proper one.''',
                                    epilog = '''After calling this command, a folder with
                                    created HeatMaps (in HDF5 format) is created under current
                                    working directory.''',
                                    formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    binReads.add_argument('-f', '--filteredDir', nargs = '*',
                          help = '''Path to the filtered HDF5 files generated by filtering
                          command. The path can point to a folder or certain files (wild cards
                          are allowed). If a folder name is provided, we will contruct a HeatMap
                          for each file in that folder.''')
    binReads.add_argument('-M', '--mode', default = 'wholeGenome',
                          choices = ['wholeGenome', 'byChromosome', 'withOverlaps'],
                          help = '''Memory usage: withOverlaps > byChromosome > wholeGenome.
                          Resolution capacity (take human genome for example):
                          withOverlaps (10kb) > byChromosome (40kb) > wholeGenome (200kb).''')
    binReads.add_argument('--includeTrans', action = 'store_true',
                          help = '''Whether to include inter-chromosomal interactions in "byChromosome"
                          mode.''')
    binReads.add_argument('-R', '--resolution', type = int, default = 200000,
                          help = 'Resolution of a heatmap. Unit: bp')
    binReads.set_defaults(func = binning)
    
    ## Iterative Correction
    iterC = subparser.add_parser('correcting',
                                 parents = [common],
                                 help = '''Perform iterative corrections on the original HeatMap.''',
                                 description = '''Two modes are provided for different resolutions.
                                 The program will choose a better one for you according to the data
                                 format.''',
                                 epilog = '''After calling this command, a folder with corrected
                                 HeatMaps (in HDF5 format) is created under current working
                                 directory.''',
                                 formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    iterC.add_argument('-H', '--HeatMap', nargs = '*',
                       help = '''Path to the HeatMap files generated by binning command. The path
                       can point to a folder or certain files (wild cards are allowed). If a folder
                       name is provided, we will perform iterative corrections for all HeatMaps in
                       that folder.''')
    iterC.set_defaults(func = correcting)
    
    ## Sparse Matrix Conversion
    sMatrix = subparser.add_parser('tosparse',
                                  parents = [common],
                                  help = '''Convert intra-chromosomal contact matrices to sparse ones.''',
                                  formatter_class = argparse.ArgumentDefaultsHelpFormatter)
    sMatrix.add_argument('-H', '--cHeatMap', nargs = '*',
                         help = '''Source HeatMap files which was saved by chromosome. Wild cards are
                         allowed. If a folder name is provided, conversion will be performed on each
                         HeatMap file under that folder.''')
    sMatrix.set_defaults(func = tosparse)
                    
    ## Pile Up
    streamline = subparser.add_parser('pileup',
                                      parents = [iterM],
                                      help = '''Perform the entire analysis from sequencing
                                      data to corrected HeatMaps.''',
                                      description = '''A more convenient but less flexible
                                      command for Hi-C data processing.''',
                                      formatter_class = argparse.ArgumentDefaultsHelpFormatter,
                                      add_help = False)
    streamline.add_argument('-M', '--mode', default = 'byChromosome',
                            choices = ['wholeGenome', 'byChromosome', 'withOverlaps'],
                            help = '''Memory usage: withOverlaps > byChromosome > wholeGenome.
                            Resolution capacity (take human genome for example):
                            withOverlaps (10kb) > byChromosome (40kb) > wholeGenome (200kb).''')
    streamline.add_argument('-R', '--resolution', type = int, default = 40000,
                          help = 'Resolution of a heatmap. Unit: bp')
    streamline.set_defaults(func = pileup)
    
     ## Parse the command-line arguments
    commands = sys.argv[1:]
    if ((not commands) or ((commands[0] in ['mapping', 'filtering', 'binning','correcting', 'pileup', 'tosparse'])
        and len(commands) == 1)):
        commands.append('-h')
    args = parser.parse_args(commands)
    
    return args, commands


def run():
    # Parse Arguments
    args, commands = getargs()
    # Improve the performance if you don't want to run it
    if commands[-1] not in ['-h', '-v', '--help', '--version']:
        # Define a special level name
        logging.addLevelName(21, 'main')
        ## Root Logger Configuration
        logger = logging.getLogger()
        # Logger Level
        logger.setLevel(21)
        filehandler = logging.handlers.RotatingFileHandler(args.logFile,
                                                           maxBytes = 100000,
                                                           backupCount = 5)
        # Set level for Handlers
        filehandler.setLevel(21)
        # Customizing Formatter
        formatter = logging.Formatter(fmt = '%(name)-20s %(levelname)-7s @ %(asctime)s: %(message)s',
                                      datefmt = '%m/%d/%y %H:%M:%S')
        ## Unified Formatter
        filehandler.setFormatter(formatter)
        # Add Handlers
        logger.addHandler(filehandler)
        ## Logging for argument setting
        arglist = ['# ARGUMENT LIST:',
                   '# Sub-Command Name = %s' % commands[0],
                   '# Data Root Directory = %s' % args.dataFolder,
                   '# Genome Name = %s' % args.genomeName,
                   '# Chromosomes = %s' % args.chroms,
                   '# FASTA template = %s' % args.template,
                   '# Gap File = %s' % args.gapFile
                   ]
        if (commands[0] == 'mapping') or (commands[0] == 'pileup'):
            arglist.extend(['# Sequencing data = %s' % args.fastqDir,
                            '# Sequencing Format = %s' % args.Format,
                            '# Bowtie2 Path = %s' % args.bowtiePath,
                            '# Bowtie2 Threads = %s' % args.threads,
                            '# MetaData = %s' % args.metadata
                            ])
            if '--bowtieIndex' in commands:
                arglist.extend(['# Bowtie2 Genome Index = %s' % args.bowtieIndex])
            arglist.extend(['# Cache Folder = %s' % args.cache])
        if commands[0] == 'filtering':
            arglist.extend(['# Source Files = %s' % args.HDF5,
                            '# MetaData = %s' % args.metadata,
                            '# Remove PCR Duplicates = %s' % args.duplicates,
                            '# Remove Same Fragments = %s' % args.sameFragments,
                            '# Remove Random Breaks = %s' % args.RandomBreaks,
                            '# Remove Extreme Fragments = %s' % args.extremeFragments,
                            '# Remove startNearRsite = %s' % args.startNearRsite,
                            '# Remove cistotrans = %s' % args.cistotrans,
                            '# Merging Level = %s' % args.level])
        if commands[0] == 'binning':
            arglist.extend(['# Source Files = %s' % args.filteredDir,
                            '# HeatMap Mode = %s' % args.mode,
                            '# HeatMap Resolution = %s' % args.resolution])
        if commands[0] == 'correcting':
            arglist.extend(['# Source HeatMap = %s' % args.HeatMap])
        
        if commands[0] == 'pileup':
            arglist.extend(['# Merging Level = 2',
                            '# Remove PCR Duplicates = True',
                            '# Remove Same Fragments = True',
                            '# Remove Random Breaks = True',
                            '# Remove Extreme Fragments = True',
                            '# Remove startNearRsite = True',
                            '# Remove cistotrans = True',
                            '# HeatMap Mode = %s' % args.mode,
                            '# HeatMap Resolution = %s' % args.resolution])
        if commands[0] == 'tosparse':
            arglist.extend(['# Source HeatMap = %s' % args.cHeatMap])
        
        argtxt = '\n'.join(arglist)
        logging.log(21, '\n' + argtxt)
            
        # Subcommand
        args.func(args, commands)

def initialize(args):
    ## Necessary Modules
    from mirnylib import genome
    ## Validity of arguments
    dataLocation = os.path.abspath(os.path.expanduser(args.dataFolder))
    if not os.path.exists(dataLocation):
        logging.error('There is no folder named %s on your system!',
                      dataLocation)
        sys.exit(1)
    genomeFolder = os.path.join(dataLocation, args.genomeName)
    if not os.path.exists(genomeFolder):
        logging.error('%s can not be found at %s', args.genomeName,
                      dataLocation)
        sys.exit(1)

    ## Generate a dummy gap file under genome folder if there's no one yet
    gapFile = os.path.join(genomeFolder, args.gapFile)
    if not os.path.exists(gapFile):
        logging.log(21, 'No gap file can be found at %s, generating a dummy one ...',
                    genomeFolder)
        tempfile = open(gapFile, 'w')
        tempfile.write('0\tNA1000\t0\t0\t0\tN\t0\tcentromere\tno\n')
        tempfile.flush()
        tempfile.close()
        logging.log(21, 'Done!')
    
    # Python Genome Object
    genome_db = genome.Genome(genomeFolder, readChrms = args.chroms)
    
    return dataLocation, genomeFolder, genome_db
    

def mapping(args, commands):
    ## Import necessary modules
    import atexit
    import hiclib.mapping as iterM
    from mirnylib import h5dict
    
     # Initialization
    dataLocation, genomeFolder, genome_db = initialize(args)
    
    # A Local Function
    def cleanFile(filename):
        if os.path.exists(filename):
            os.remove(filename)
    
    # Construct bowtie2 genome index
    def buildIndex(genomeFolder):
        """
        Build bowtie2 index files under the provided genome folder.
        
        """
        fastaNames = [os.path.join(genomeFolder, i)
                      for i in glob.glob(os.path.join(
                      genomeFolder, args.template % ('*',)))]
        wholeGenome = os.path.join(genomeFolder,
                                   '.'.join([args.genomeName, 'fa']))
        if not os.path.exists(wholeGenome):
            os.system('cat ' + ' '.join(fastaNames) + ' > ' + wholeGenome)
        bowtieIndex = os.path.join(genomeFolder, args.genomeName)
        buildCmd = ['bowtie2-build', '--quiet', wholeGenome, bowtieIndex]
        os.system(' '.join(buildCmd))
        
        return bowtieIndex
    
    def calculateStep(length, minlen, approxStep=10, maxSteps=4):
        """
        Returns minimum length and step based on the length of sequence and
        proposed minimum length.
        """
        actualDif = length - minlen
        if actualDif < approxStep * 0.6:
            return length, 100

        numIter = np.array(np.around(actualDif / float(approxStep)), dtype=int)
        if numIter == 0:
            numIter = 1
        if numIter > maxSteps:
            numIter = maxSteps
        actualStep = actualDif / numIter

        minlen = length - actualStep * numIter

        return minlen, actualStep
    
    ## Validity of arguments
    bowtiePath = os.path.abspath(os.path.expanduser(args.bowtiePath))
    if not os.path.exists(bowtiePath):
        logging.error('Bowtie2 can not be found at %s', bowtiePath)
        sys.exit(1)
    fastqDir = os.path.join(dataLocation, args.fastqDir)
    if not os.path.exists(fastqDir):
        logging.error('%s should be placed under %s', args.fastqDir, dataLocation)
        sys.exit(1)
    mFile = args.metadata
    if not os.path.exists(mFile):
        logging.error('Metadata file %s can not be found at current working directory!',
                      mFile)
        sys.exit(1)
    cache = os.path.abspath(os.path.expanduser(args.cache))
    if not os.path.exists(cache):
        logging.warning('%s does not exist on your system, trying to create one',
                        cache)
        os.makedirs(cache)
    
    ## Construct bowtie2 genome index if there's no one yet
    if '--bowtieIndex' in commands:
        bowtieIndex = os.path.abspath(os.path.expanduser(args.bowtieIndex))
    else:
        logging.log(21, 'You haven\'t specify the Bowtie2 Genome Index Files.')
        logging.log(21, 'Try to find them at %s ...', genomeFolder)
        icheck = glob.glob(os.path.join(genomeFolder, '%s*.bt2' % args.genomeName))
        if len(icheck) != 0:
            logging.log(21, 'Index files are found at %s', genomeFolder)
            bowtieIndex = os.path.join(genomeFolder, args.genomeName)
            logging.log(21, 'Set --bowtieIndex to %s', bowtieIndex)
        else:
            logging.log(21, 'Index files can not be found. Generating them under the'
                        ' genome folder ...')
            bowtieIndex = buildIndex(genomeFolder)
            logging.log(21, 'Done!')
            
    logging.log(21, 'Now, extract read pairs from %s files and map them to %s',
                args.Format, args.genomeName)
    ## Sequencing Data Format
    Format = args.Format.lower()
    sraNames = [os.path.join(fastqDir, i) for i in glob.glob(os.path.join(
                fastqDir, '%s.%s' % ('*', Format)))]
    ## Sequencing Length
    lengths = os.path.join(fastqDir, 'lengths')
    if not os.path.exists(lengths):
        os.mkdir(lengths)
    if Format == 'sra':
        Set = set([os.path.basename(i)[:-4] for i in sraNames])
        for i in sraNames:
            calLength = ['fastq-dump', '-Z', i, '|', 'head', '-n', '2',
                         '|', 'tail', '-n', '1', '|', 'wc', '-c', '>',
                         os.path.join(lengths, os.path.basename(i)[:-4])]
            os.system(' '.join(calLength))
    else:
        Set = set([os.path.basename(i)[:-8] for i in sraNames])
        for i in Set:
            leftSide = os.path.join(fastqDir, i + '_1.fastq')
            calLength = ['head', '-n', '2', leftSide, '|', 'tail', '-n', '1', '|',
                         'wc', '-c', '>', os.path.join(lengths, i)]
            os.system(' '.join(calLength))
    
    ## Output Folders
    bamFolder = 'bams-%s' % args.genomeName
    hdf5F = 'hdf5-%s' % args.genomeName
    args.HDF5 = hdf5F # To communicate with next processing step (merge)
    if not os.path.exists(bamFolder):
        os.mkdir(bamFolder)
    if not os.path.exists(hdf5F):
        os.mkdir(hdf5F)
    
    logging.log(21, 'Bowtie2 alignment results will be saved in bam format under %s',
                bamFolder)
    logging.log(21, 'Bam files will be parsed into hdf5 format under %s', hdf5F)
    
    # Read Metadata
    metadata = [l.rstrip().split() for l in open(mFile)]
    database = dict([(i[0], i[-1]) for i in metadata])
    for i in sorted(list(Set)):
        if i in database:
            logging.log(21, 'Current %s file: %s', args.Format, i)
        
            finalFile = os.path.join(hdf5F, '%s.hdf5' % i)
            lockFile = os.path.join(hdf5F, '%s.lock' % i)
        
            if os.path.exists(finalFile) and not os.path.exists(lockFile):
                logging.log(21, '%s already exists, skipping', finalFile)
                continue
        
            if os.path.exists(lockFile):
                logging.log(21, 'Someone is working on %s, skipping', finalFile)
                continue
        
            # Parameters used in iterative mapping
            lengthFile = os.path.join(lengths, i)
            if Format == 'sra':
                length = (int(open(lengthFile).readlines()[0]) - 1) / 2
            else:
                length = int(open(lengthFile).readlines()[0])
            logging.log(21, 'Extract sequence length ... %s', length)
            logging.log(21, 'Determining parameters for iterative mapping ...')
            minlen, step = calculateStep(length, 25)
            logging.log(21, 'minlen = %s, step = %s', minlen, step)
        
            logging.log(21, 'Create %s to ensure process safety ...', lockFile)
            lock = open(lockFile, 'w')
            lock.close()
            logging.log(21, '%s will be removed if the program terminates normally.', lockFile)
        
            atexit.register(cleanFile, lockFile)
        
            cleanup = ['rm', '-rf', os.path.join(bamFolder, '%s*' % i)]
            os.system(' '.join(cleanup))
        
            ## Iterative Mapping
            # Common Parameters
            Parameters = {'bowtie_path': bowtiePath, 'bowtie_index_path': bowtieIndex,
                          'min_seq_len': minlen, 'len_step': step, 'nthreads': args.threads,
                          'temp_dir': cache, 'bowtie_flags': '--very-sensitive'}
            if Format == 'sra':
                sourceFile = os.path.join(fastqDir, i + '.sra')
                # The First Side
                logging.log(21, 'Mapping first side of the reads ...')
                iterM.iterative_mapping(fastq_path = sourceFile,
                                        out_sam_path = '%s/%s_1.bam' % (bamFolder, i),
                                        seq_start = 0,
                                        seq_end = length,
                                        bash_reader = 'fastq-dump -Z',
                                        **Parameters)
                logging.log(21, 'Done!')
                logging.log(21, 'Mapping second side of the reads ...')
                # The Second Side
                iterM.iterative_mapping(fastq_path = sourceFile,
                                        out_sam_path = '%s/%s_2.bam' % (bamFolder, i),
                                        seq_start = length,
                                        seq_end = 2 * length,
                                        bash_reader = 'fastq-dump -Z',
                                        **Parameters)
                logging.log(21, 'Done!')
            else:
                logging.log(21, 'Mapping first side of the reads ...')
                iterM.iterative_mapping(fastq_path = os.path.join(fastqDir, i + '_1.fastq'),
                                        out_sam_path = '%s/%s_1.bam' % (bamFolder, i),
                                        **Parameters)
                logging.log(21, 'Done!')
                logging.log(21, 'Mapping second side of the reads ...')
                iterM.iterative_mapping(fastq_path = os.path.join(fastqDir, i + '_2.fastq'),
                                        out_sam_path = '%s/%s_2.bam' % (bamFolder, i),
                                        **Parameters)
                logging.log(21, 'Done!')
        
            logging.log(21, 'Parsing mapped sequences ...')
            ## Parse the mapped sequences into a Python data structure
            ## Assign the ultra-sonic fragments to restriction fragments
            lib = h5dict.h5dict(finalFile)
            iterM.parse_sam(sam_basename1 = '%s/%s_1.bam' % (bamFolder, i),
                            sam_basename2 = '%s/%s_2.bam' % (bamFolder, i),
                            out_dict = lib,
                            genome_db = genome_db,
                            enzyme_name = database[i],
                            save_seqs = False)
            logging.log(21, 'Done!')
        
            os.remove(lockFile)

def filtering(args, commands):
    # Necessary Modules
    from runHiC.utilities import cHiCdataset
    
    ## Validity of arguments
    Sources = os.path.abspath(os.path.expanduser(args.HDF5))
    if not os.path.exists(Sources):
        logging.error('There is no folder named %s on your system!', Sources)
        sys.exit(1)
    mFile = args.metadata
    if not os.path.exists(mFile):
        logging.error('%s can not be found under current working directory!', mFile)
        sys.exit(1)
    
    logging.log(21, 'According to %s, perform filtering and merging processes on hdf5 files under %s',
                args.metadata, Sources)
    # Output Folder
    filteredFolder = 'filtered-%s' % args.genomeName
    if not os.path.exists(filteredFolder):
        os.mkdir(filteredFolder)
    args.filteredDir = [] # To communicate with next processing step (binning)
    
    logging.log(21, 'Filtered files will be saved under %s', filteredFolder)
    
    metadata = [l.rstrip().split() for l in open(mFile)]
    ## Hierarchical merging structures
    bioReps = set((i[1], i[3], i[2]) for i in metadata)
    cellLines = set((i[1], i[3]) for i in metadata)
    
    ## The First level, biological replicates
    logging.log(21, 'Filtering data from the same biological replicate ...')
    queueL1 = []
    for rep in bioReps:
        filenames = [os.path.join(Sources, '%s.hdf5' % i[0]) for i in metadata
                    if ((i[1], i[3], i[2]) == rep)]
        outfile = os.path.join(filteredFolder, '%s-%s-%s-filtered.hdf5' % rep)
        args.filteredDir.append(outfile)
        enzyme = rep[1]
        queueL1.append((filenames, outfile, enzyme))
    for member in queueL1:
        # Initialize a Genome Object
        dataLocation, genomeFolder, genome_db = initialize(args)
        genome_db.setEnzyme(member[-1])
        lanePools = []
        for source in member[0]:
            parseName = os.path.join(filteredFolder, '%s-parsed.hdf5' % os.path.basename(source).replace('.hdf5', ''))
            ## Create a cHiCdataset object
            parseF = cHiCdataset(filename = parseName,
                                 genome = genome_db,
                                 mode = 'w')
            parseF.parseInputData(source, commandArgs = args)
            lanePools.append(parseName)
        ## Merge lane data altogether
        fragments = cHiCdataset(filename = member[1], genome = genome_db, mode = 'w')
        fragments.merge(lanePools)
        ## Additional filtering
        if args.duplicates:
            fragments.filterDuplicates()
        if args.startNearRsite:
            fragments.filterRsiteStart(offset=5)
        if args.extremeFragments:
            fragments.filterLarge()
        if args.cistotrans:
            fragments.filterExtreme(cutH=0.005, cutL=0)
            
        # Clean up parsed individual files
        for delFile in lanePools:
            os.remove(delFile)
    logging.log(21, 'Done!')
    
    if args.level == 2:
        ## The Second level, cell lines, optional
        logging.log(21, 'Merging data of the same cell line using the same restriction enzyme ...')
        
        queueL2 = []
        for cell in cellLines:
            filenames = [os.path.join(filteredFolder, '%s-%s-%s-filtered.hdf5' % i) for i in bioReps
                         if ((i[0], i[1]) == cell)]
            outfile = os.path.join(filteredFolder, '%s-%s-allReps-filtered.hdf5' % cell)
            args.filteredDir.append(outfile)
            enzyme = cell[-1]
            queueL2.append((filenames, outfile, enzyme))
        
        for member in queueL2:
            # Initialize a Genome Object
            dataLocation, genomeFolder, genome_db = initialize(args)
            genome_db.setEnzyme(member[-1])
            fragments = cHiCdataset(filename = member[1], genome = genome_db, mode = 'w')
            fragments.merge(member[0])
    logging.log(21, 'Done!')

def binning(args, commands):
    # Necessary Modules
    from runHiC.utilities import cHiCdataset
    
    Sources = [os.path.abspath(os.path.expanduser(i)) for i in args.filteredDir]
    
    # Output Dir
    hFolder = 'Heatmaps-%s' % args.genomeName
    if not os.path.exists(hFolder):
        os.mkdir(hFolder)
    # To communicate with next processing step (correcting)
    args.HeatMap = []
    
    logging.log(21, 'HeatMaps will be saved in hdf5 format under %s', hFolder)
    
    ## Generate HeatMaps
    for S in Sources:
        if os.path.isdir(S):
            queue = [os.path.join(S, i) for i in glob.glob(os.path.join(S, '*-filtered.hdf5'))]
            if len(queue) == 0:
                logging.warning('No proper files can be found at %s, suffix "-filtered.hdf5" is required!', S)
            logging.log(21, 'Generate HeatMaps based on filtered hdf5 files under %s ...', S)
        else:
            parse = os.path.split(S)
            sFolder = parse[0]
            mode = parse[1]
            logging.log(21, 'Finding matched files (%s) under %s ...', mode, sFolder)
            queue = [os.path.join(sFolder, i) for i in glob.glob(S) if i.endswith('-filtered.hdf5')]
            if len(queue) == 0:
                logging.warning('No matched file under %s, suffix "-filtered.hdf5" is required!', sFolder)
            logging.log(21, 'Generate HeatMaps for all matched files under %s ...', sFolder)
    
        # Appropriate Units
        unit, denominator = ('K', 1000) if (args.resolution / 1000 < 1000) else ('M', 1000000)
        nLabel = str(args.resolution / denominator) + unit
        for f in queue:
            logging.log(21, 'Current source file: %s', f)
            hFile = os.path.join(hFolder, os.path.basename(f).replace('.hdf5', '-%s.hm' % nLabel))
            args.HeatMap.append(hFile)
            # Parse restriction enzyme name from the file name
            enzyme = os.path.basename(f).split('-')[1]
            # Initialize a Genome Object
            dataLocation, genomeFolder, genome_db = initialize(args)
            genome_db.setEnzyme(enzyme)
            fragments = cHiCdataset(f, genome = genome_db, mode = 'r')
            ## Different Modes
            if args.mode == 'wholeGenome':
                fragments.saveHeatmap(hFile, resolution = args.resolution)
            if args.mode == 'byChromosome':
                fragments.saveByChromosomeHeatmap(hFile, resolution = args.resolution, includeTrans = args.includeTrans)
            if args.mode == 'withOverlaps':
                fragments.saveHiResHeatmapWithOverlaps(hFile, resolution = args.resolution)
    
    logging.log(21, 'Done!')

def correcting(args, commands):
    ## Necessary Modules
    from mirnylib import h5dict
    
    # Initialization
    dataLocation, genomeFolder, genome_db = initialize(args)
    
    ## Two modes
    def Lcore(inFile, outFile, resolution):
        # Necessary Modules
        from hiclib import binnedData
        # Create a binnedData object, load the data.
        logging.log(21, 'Loading Contact Matrix of the Whole Genome ...')
        BD = binnedData.binnedData(resolution, genome_db)
        name = '-'.join(os.path.basename(inFile).split('-')[:3])
        BD.simpleLoad(inFile, name)
        logging.log(21, 'Done!')
        ## Perform ICE
        # Remove the contacts between loci located within the same bin.
        BD.removeDiagonal()
        # Remove bins with less than half of a bin sequenced.
        BD.removeBySequencedCount(0.5)
        # Remove 0.5% bins with the lowest number of records
        BD.removePoorRegions(cutoff = 0.5, coverage = True)
        BD.removePoorRegions(cutoff = 0.5, coverage = False)
        # Truncate top 0.05% of inter-chromosomal counts (possibly, PCR blowouts).
        BD.truncTrans(high = 0.0005)
        # Perform iterative correction.
        logging.log(21, 'Perform ICE ...')
        BD.iterativeCorrectWithoutSS()
        logging.log(21, 'Done!')
        # Save the iteratively corrected heatmap.
        logging.log(21, 'Exporting Corrected Matrix ...')
        BD.export(name, outFile)
        logging.log(21, 'Done!')

    def Hcore(inFile, outFile, resolution):
        # Necessary Modules
        from mirnylib.numutils import completeIC
        # Create a hdf5 file manually
        mydict = h5dict.h5dict(outFile)
        # Raw HeatMap
        raw = h5dict.h5dict(inFile, mode = 'r')
        ## Perform ICE for each chsomosome
        keys = raw.keys()
        cisKeys = [i for i in keys if ((len(set(i.split())) == 1) and (i != 'resolution'))]
        for i in cisKeys:
            logging.log(21, 'Current Chromosome ID: %s', i.split()[0])
            if i in mydict.keys():
                logging.log(21, '"%s" is already there, skipping ...', i)
                continue
            logging.log(21, 'Loading Cis-HeatMap: "%s" from %s ...', i, inFile)
            # Extract Cis HeatMap
            rawHeatMap = raw[i]
            logging.log(21, 'Done!')
            # Only work for cis-heatmaps
            logging.log(21, 'Correcting ...')
            cHeatMap, bias = completeIC(rawHeatMap, returnBias = True)
            logging.log(21, 'Done!')
            logging.log(21, 'Writing Corrected Matrix to %s ...', outFile)
            mydict[i] = cHeatMap
            logging.log(21, 'Done!')
        mydict['resolution'] = resolution
        
    Sources = [os.path.abspath(os.path.expanduser(i)) for i in args.HeatMap]
    
    ## Output Dir
    cFolder = 'Corrected-%s' % args.genomeName
    if not os.path.exists(cFolder):
        os.mkdir(cFolder)
    
    # To communicate with next processing step
    args.cHeatMap = []
    
    logging.log(21, 'Corrected HeatMaps will be generated under %s', cFolder)
    
    ## Corrections start
    for S in Sources:
        if os.path.isdir(S):
            queue = [os.path.join(S, i) for i in glob.glob(os.path.join(S, '*.hm'))]
            if len(queue) == 0:
                logging.warning('No proper files can be found at %s, suffix ".hm" is required!', S)
            logging.log(21, 'Correcting all HeatMaps under %s ...', S)
        else:
            parse = os.path.split(S)
            sFolder = parse[0]
            mode = parse[1]
            logging.log(21, 'Finding matched files (%s) under %s ...', mode, sFolder)
            queue = [os.path.join(sFolder, i) for i in glob.glob(S) if i.endswith('.hm')]
            if len(queue) == 0:
                logging.warning('No matched file under %s, suffix ".hm" is required!', sFolder)
            logging.log(21, 'Correcting all matched HeatMaps under %s ...', sFolder)
    
        for f in queue:
            logging.log(21, 'Current source file: %s', f)
            # Output file
            cFile = os.path.join(cFolder, os.path.basename(f).replace('.hm', '_c.hm'))
            args.cHeatMap.append(cFile)
            # Raw Data
            raw = h5dict.h5dict(f, mode = 'r')
            Keys = raw.keys()
            resolution = int(raw['resolution'])
            if 'heatmap' in Keys: # Low resolution case
                Lcore(f, cFile, resolution)
            else: # High resolution case
                try:
                    Hcore(f, cFile, resolution)
                except ValueError:
                    logging.warning('Iterative correction will remove more than a half of the matrix')
                    logging.warning('Skipping ...')
                    continue

def tosparse(args, commands):
    # Necessary Modules
    from mirnylib import h5dict
    from runHiC.utilities import toSparse
    
    # Initialization
    dataLocation, genomeFolder, genome_db = initialize(args)
    
    Sources = [os.path.abspath(os.path.expanduser(i)) for i in args.cHeatMap]
    
    for S in Sources:
        if os.path.isdir(S):
            queue = [os.path.join(S, i) for i in glob.glob(os.path.join(S, '*.hm'))]
            if len(queue) == 0:
                logging.warning('No proper files can be found at %s, suffix ".hm" is required!', S)
            logging.log(21, 'Converting all by-chromosome HeatMaps under %s ...', S)
        else:
            parse = os.path.split(S)
            sFolder = parse[0]
            mode = parse[1]
            logging.log(21, 'Finding matched files (%s) under %s ...', mode, sFolder)
            queue = [os.path.join(sFolder, i) for i in glob.glob(S) if i.endswith('.hm')]
            if len(queue) == 0:
                logging.warning('No matched file under %s, suffix ".hm" is required!', sFolder)
            logging.log(21, 'Converting all matched HeatMaps under %s ...', sFolder)
        
        for f in queue:
            logging.log(21, 'Current HeatMap file: %s', f)
            # Check if it is by-chromosome HeatMap
            raw = h5dict.h5dict(f, mode = 'r')
            Keys = raw.keys()
            if 'heatmap' in Keys:
                logging.log(21, '%s is a whole-genome HeatMap', f)
                logging.log(21, 'Skipping ...')
            else:
                toSparse(source = f, idx2label = genome_db.idx2label)
    
            
def pileup(args, commands):
    """
    A customized pipeline covering the whole process.
    
    """
    mapping(args, commands)
    args.level = 2
    args.duplicates = args.sameFragments = args.startNearRsite = True
    args.extremeFragments = True
    args.RandomBreaks = args.cistotrans = True
    filtering(args, commands)
    args.includeTrans = False
    binning(args, commands)
    correcting(args, commands)
    if args.mode == 'wholeGenome':
        logging.error('Only by-chromosome HeatMaps can be converted to sparse ones!')
        logging.error('Exit ...')
        sys.exit(1)
    tosparse(args, commands)
    

if __name__ == '__main__':
    run()
